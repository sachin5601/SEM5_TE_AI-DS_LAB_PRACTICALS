{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKBFwWh23ym_"
   },
   "source": [
    "Exp-08 (Apriori Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIkWY7l4-ur1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# Load CSV file and preprocess the data\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    transactions = []\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for i in range(len(data)):\n",
    "        transaction = []\n",
    "        # Check for each item column (item1, item2, item3, item4, item5)\n",
    "        for j in range(1, len(data.columns)):\n",
    "            item = data.iloc[i, j]\n",
    "            if pd.notna(item):  # Check if the item is not NaN\n",
    "                transaction.append(item)\n",
    "        transactions.append(transaction)\n",
    "\n",
    "    return transactions\n",
    "\n",
    "# Calculate the support of itemsets\n",
    "def calculate_support(transactions, itemsets):\n",
    "    support = {}\n",
    "    for itemset in itemsets:\n",
    "        itemset_tuple = tuple(itemset)\n",
    "        support_count = sum([1 for transaction in transactions if set(itemset).issubset(set(transaction))])\n",
    "        support[itemset_tuple] = support_count / len(transactions)\n",
    "    return support\n",
    "\n",
    "# Prune itemsets that do not meet the minimum support\n",
    "def prune_itemsets(support, min_support):\n",
    "    return {itemset: support_val for itemset, support_val in support.items() if support_val >= min_support}\n",
    "\n",
    "# Generate candidate itemsets of size k+1 from frequent itemsets of size k\n",
    "def generate_candidates(frequent_itemsets, k):\n",
    "    candidates = set()\n",
    "    frequent_items = list(frequent_itemsets.keys())\n",
    "\n",
    "    for i in range(len(frequent_items)):\n",
    "        for j in range(i + 1, len(frequent_items)):\n",
    "            union_set = set(frequent_items[i]).union(frequent_items[j])\n",
    "            if len(union_set) == k + 1:\n",
    "                candidates.add(tuple(sorted(union_set)))\n",
    "\n",
    "    return candidates\n",
    "\n",
    "# Calculate confidence for association rules\n",
    "def calculate_confidence(frequent_itemsets, transactions, min_confidence):\n",
    "    rules = []\n",
    "\n",
    "    for itemset in frequent_itemsets:\n",
    "        if len(itemset) > 1:\n",
    "            subsets = list(combinations(itemset, len(itemset) - 1))\n",
    "\n",
    "            for subset in subsets:\n",
    "                remaining = tuple(set(itemset) - set(subset))\n",
    "                subset_support = sum([1 for transaction in transactions if set(subset).issubset(set(transaction))]) / len(transactions)\n",
    "                itemset_support = frequent_itemsets[itemset]\n",
    "\n",
    "                confidence = itemset_support / subset_support if subset_support > 0 else 0\n",
    "\n",
    "                # Convert confidence to percentage\n",
    "                confidence_percentage = confidence * 100\n",
    "\n",
    "                if confidence >= min_confidence:\n",
    "                    rules.append((subset, remaining, confidence_percentage))\n",
    "\n",
    "    return rules\n",
    "\n",
    "# Apriori algorithm\n",
    "def apriori(transactions, min_support, min_confidence):\n",
    "    # Generate 1-itemsets\n",
    "    itemsets = [{item} for transaction in transactions for item in transaction]\n",
    "    itemsets = [list(x) for x in set(tuple(sorted(x)) for x in itemsets)]\n",
    "\n",
    "    # Calculate initial support for 1-itemsets\n",
    "    support = calculate_support(transactions, itemsets)\n",
    "\n",
    "    # Filter out itemsets that do not meet minimum support\n",
    "    frequent_itemsets = prune_itemsets(support, min_support)\n",
    "\n",
    "    all_frequent_itemsets = frequent_itemsets.copy()\n",
    "    k = 1\n",
    "\n",
    "    while frequent_itemsets:\n",
    "        # Generate candidate itemsets of size k+1\n",
    "        candidates = generate_candidates(frequent_itemsets, k)\n",
    "\n",
    "        # Calculate support for candidate itemsets\n",
    "        support = calculate_support(transactions, candidates)\n",
    "\n",
    "        # Prune itemsets that do not meet the minimum support\n",
    "        frequent_itemsets = prune_itemsets(support, min_support)\n",
    "\n",
    "        # Add frequent itemsets to the global list\n",
    "        all_frequent_itemsets.update(frequent_itemsets)\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    # Calculate confidence for association rules\n",
    "    rules = calculate_confidence(all_frequent_itemsets, transactions, min_confidence)\n",
    "\n",
    "    return all_frequent_itemsets, rules\n",
    "\n",
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    file_path = 'aprior.csv'  # Ensure this matches your actual file path\n",
    "    min_support = 0.3\n",
    "    min_confidence_percentage = 70  # Minimum confidence in percentage\n",
    "\n",
    "    # Convert percentage to decimal for calculations\n",
    "    min_confidence = min_confidence_percentage / 100.0\n",
    "\n",
    "    # Load transactions\n",
    "    transactions = load_data(file_path)\n",
    "\n",
    "    # Run Apriori algorithm\n",
    "    frequent_itemsets, rules = apriori(transactions, min_support, min_confidence)\n",
    "\n",
    "    # Output Frequent Itemsets\n",
    "    print(\"Frequent Itemsets:\")\n",
    "    for itemset, support in frequent_itemsets.items():\n",
    "        print(f\"{itemset}: {support:.2f}\")\n",
    "\n",
    "    # Output Association Rules with Confidence in Percentage\n",
    "    print(\"\\nAssociation Rules:\")\n",
    "    for rule in rules:\n",
    "        antecedent, consequent, confidence = rule\n",
    "        print(f\"{antecedent} -> {consequent}: Confidence = {confidence:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
